# Mini_Text_Generative_Model
Developed a mini generative language model using PyTorch for character-level text data, utilizing components like multi-head self-attention, transformer blocks, and a bigram language model. Implemented key deep learning techniques such as token embeddings, feed-forward neural networks, and cross-entropy loss optimization to generate text sequences.
